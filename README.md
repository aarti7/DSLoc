# Doppler spread based localization 
We implement the following strategy for generating the fingerprint database `F`. 

We perform a series of large-scale, over-the- air experiments on the POWDER platform. In total `N=5` base-stations ( namely Behavioral, Honors, Hospital, SMT, UStar) are utilized.  

### Experiments

A mobile node continuously transmits an unmodulated, complex sinusoidal wave at a frequency `f_c = 3515 MHz`. The bandwidth of operation is 10 MHz. The B210 radios that are used for transmission, itself do not have a fixed power level of transmission, as the power level is determined by the RF front-end and the antenna used with the SDR. The AD9361 RFIC transceiver used in the B210 provides an adjustable and programmable output power ranging from -4 dBm to 6 dBm.

 Multiple base-stations simultaneously record the IQ values during the duration of each of the experiments. Each base-station has a USRP X310 receiver. The power of operation is and the receive gain is maintained as 30 dB. All the receiver X310 have an external clock of base-stations are time and frequency synchronized with the White Rabbit Synchronization system.



### Install the required packages:

1. Following python packages needs to be installed: 
	* numpy 
	* pandas 
	* scipy
	* h5py
	* pytz
	* matplotlib
	* utm
	
2.  To install the above packages, open the terminal and run: `pip install -r requirements.txt`.



###  List of scripts:
1. `hdf5_to_pickles.py`: 
2. `functions_cforemoval.py`: 
3. `pickles_to_spectrum_postprocessing.py`:

* Additional scripts: `libs.py`: has the needed library via `imports`. 



### Hdf5 to pandas dataframe conversion
* Go to the common data directory that has all the experiments folders generated by SHOUT. The naming convention used is `Shout_meas_<date>_<mountaintime>` For example: `Shout_meas_01-30-2023_15-40-55`. 
* Each folder must have these files:
     1. \<THIS\_SHOUT\_EXPERIMENTS\_SETTINGS\_FILE\>.json
     2. bus-\<ROUTENUMBER\>\_locations.csv
     3. log
     4. measurements.hdf5

* Run the  `hdf5_to_pickles.py` script as  `python hdf5_to_pickles.py`: It extracts the IQ data from the original hdf5 files and put them into pandas' dataframes. Also, the ground truth location from the .csv file are read through this script.

   
* 		List of functions in the `hdf5_to_pickles.py` script:
	  		1. `parse_args_def` 
     		2. `leaves_to_DF`
     		3. `do_data_storing`
     		4. `get_dataset_keys`
     		5. `parse_args_def`
     
* 		List of functions in the `functions_cforemoval.py` script:
     		1. `get_full_DS_spectrum`
     		2. `fit_freq_on_time`
     		3. `mylpf`
     		4. `get_cfo`
     		5. `convert_strptime_to_currUTCtsdelta`
     		6. `get_interpolated_foff`
     		7. `do_cfo_removal`

   
* For each experiment, one pickle file gets stored in the common data directory.
<!--*  and not in the git directory to avoid branch updates. --> 
Naming convention maintained is `Shout_meas_<date>_<mountaintime>.pickle`. 
For example: `Shout_meas_01-30-2023_15-40-55.pickle`

* The `pickles_to_spectrum_postprocessing.py` script plots the  power spectrum densities (psds). It can be seemlessly run after all the pickle files are done getting generated _iff_ the flag -m is set to 1. (Default is 0)




## Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant Numbers 2232464, 1827940, and 1622741.
